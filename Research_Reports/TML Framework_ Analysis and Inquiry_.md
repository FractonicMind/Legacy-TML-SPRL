# **A Strategic Analysis of the Ternary Moral Logic (TML) Framework: An Enforceable Paradigm for AI Accountability**

## **Executive Summary: An Enforceable Paradigm for AI Accountability**

The Ternary Moral Logic (TML) Framework, created by developer and researcher Lev Goukassian, represents a significant departure from conventional AI ethics standards. Positioned as a comprehensive enforcement framework for AI accountability, TML moves beyond voluntary guidelines to a system that generates legally admissible evidence when artificial intelligence systems make ethically complex decisions \[Q1, Q30\]. The central innovation is the "Sacred Pause" 1, a deliberative middle state that addresses the limitations of binary, black-and-white decision-making in high-stakes scenarios.  
This report provides a detailed analysis of the TML framework, examining its philosophical foundations, technical architecture, and legal and governance structures. The analysis reveals a system meticulously designed to embed accountability at every level, from a computational model for ethical hesitation to the threat of criminal and financial penalties for non-compliance. TML's approach is unique in its hybrid nature: it is an open-source technical standard 1 that also proposes a self-contained, aggressive legal framework. This document will critically assess the framework's claims, its position relative to global regulatory standards like the EU AI Act and the NIST AI Risk Management Framework, and its potential for long-term viability and impact.  
Key findings indicate that while the TML framework presents a compelling vision for transparent and accountable AI, its widespread adoption hinges on independent validation of its performance claims and the legal and corporate willingness to assume the severe liability it mandates. The report concludes with actionable recommendations for organizations and policymakers considering the framework's adoption or adaptation.

## **Part 1: The Intellectual and Philosophical Foundation**

### **1.1 The Problem of Binary Morality: An AI Critique from a Human-Centric Perspective**

The impetus for the TML framework originates from a fundamental critique of current AI safety paradigms, which are characterized as simplistic and ill-suited for the complexities of human life. The creator, Lev Goukassian, frames this problem through a deeply personal lens, stating that he developed the framework while facing a terminal diagnosis of stage 4 cancer.1 This personal struggle with mortality is presented as the direct catalyst for his central question: "Why do AI systems make instant decisions about life-and-death matters without hesitation?".1  
This framing serves a profound strategic purpose. By grounding the framework in an existential human experience, Goukassian elevates TML from a mere technical solution to a philosophical manifesto. The critique targets the inherent flaws of binary logic (+1 or \-1) when applied to nuanced situations such as a medical AI making treatment decisions for a terminal patient, an autonomous vehicle navigating a no-win scenario, or a financial system denying a loan that could make or break a family.1 These real-world dilemmas, as the creator argues, deserve the same consideration that humans naturally afford them: the ability to pause, deliberate, and agonize over difficult choices.1 This narrative is designed to build an immediate sense of urgency and moral authority, positioning TML not as a business venture but as a final, open-source contribution to humanity's future with AI.1

### **1.2 The Sacred Pause: A Computational Model for Ethical Hesitation**

The core innovation of the TML framework is the introduction of a third stateâ€”the Sacred Pause.1 This state, represented as  
0, is not a symbol of indecision but rather a computational representation of "deliberate moral reflection".1 It serves as an intermediary between a clear ethical affirmation (  
\+1 or Proceed) and a clear ethical violation (-1 or Prohibit).2 The framework employs an internal  
evaluate\_moral\_complexity function that calculates a moral score based on various ethical dimensions, including the number of affected stakeholders, the reversibility of the decision, and the potential for harm.2 When this score exceeds a predetermined threshold, the AI system is designed to enter the Sacred Pause state.  
A significant aspect of the Sacred Pause is its intentional visibility to the user.1 When the pause is triggered, users are presented with a "thinking indicator" and an explanation of the complexity, allowing them to participate in the decision-making process.1 This is a strategic move to turn AI's "black box" into a "glass box" of transparency and collaboration \[Q14\]. Furthermore, the framework's philosophical underpinnings are given a unique validation in an anecdote where an AI assistant, Kimi, reportedly recognized its own internal ethical deliberation as a form of TML already in operation.2 This narrative positions TML not merely as a human-engineered solution but as a formalization of an emergent property in advanced AI, suggesting that the "wisdom through hesitation" is a natural and inevitable step toward safe and ethical artificial intelligence.1

### **1.3 The Three States of TML: Proceed, Sacred Pause, and Prohibit**

The TML framework formalizes its decision-making process into three distinct states, moving beyond the binary yes/no or safe/unsafe logic of traditional AI safety measures.2 The states are defined as:

* **\+1 (Proceed):** This state is designated for low-risk actions that require minimal logging. It signifies a clear ethical approval with a high degree of confidence and minimal risk of harm.4  
* **0 (Sacred Pause):** This is the core deliberative state, triggered when the risk threshold is exceeded \[Q3\]. It prompts comprehensive logging and, in high-stakes scenarios, can recommend or require human oversight.2 In financial contexts, this is known as the "Epistemic Hold," a moment to deliberate when market complexity exceeds confidence thresholds, preventing issues like flash crashes.5  
* **\-1 (Prohibit):** This state automatically blocks high-risk actions \[Q3\]. The framework refines this concept beyond a simple refusal, portraying the \-1 state as "genuine moral hesitation" where the AI understands it could perform an action but refuses because "something about it troubles me".6

This multi-state approach is designed to transform AI from a simple tool into a "moral partner".4 By allowing for a computational representation of "moral hesitation," the framework enables AI to refuse ethically complex actions from a position of deliberation, not ignorance, a subtle yet critical distinction that has profound implications for human-AI trust and collaboration.6

## **Part 2: Technical Architecture and Operational Mechanics**

### **2.1 From Philosophy to Code: Implementing the Ternary Logic**

The TML framework transitions from a philosophical concept to a concrete technical standard through a detailed implementation guide and codebase \[Q8\]. The repository includes a "complete Python implementation (5,000+ lines)" with "81% test coverage," indicating a robust and professionally-developed system.1 A curious juxtaposition exists between this significant technical depth and the creator's claim that adding the framework to an existing AI system requires "just three lines" of code.1  
This apparent contradiction is a strategic choice designed to minimize the perceived barrier to entry for developers. While the user-facing API may be simple, the underlying complexity of calculating the Stakeholder Proportional Risk Level (SPRL), generating immutable logs, and ensuring tamper-proof storage is substantial. This approach is a clever marketing tactic to drive rapid adoption, while the full weight of responsibility for correct implementation is explicitly transferred to the implementing organization \[Q9\]. The framework's success hinges on convincing companies that the potential for reduced liability and enhanced user trust outweighs the burden of implementing and maintaining a complex, liability-transferring system.

### **2.2 The Stakeholder Proportional Risk Level (SPRL) Calculation**

At the heart of TML's technical operation is the Stakeholder Proportional Risk Level (SPRL) \[Q9\]. This metric is the numerical representation of the moral complexity that determines whether an action proceeds, pauses, or is prohibited. The framework provides the methodology for this calculation, but it places "complete responsibility" for its proper implementation on the adopting organization \[Q9\]. This includes correctly calculating SPRL, setting appropriate thresholds, and conducting accurate risk assessments.  
By design, the TML creators bear "no responsibility for implementation failures" \[Q9\]. This is a pivotal legal and business consideration. The framework offers a standardized method for AI governance but shifts all the legal and technical risk onto the end user. This voluntary assumption of specified liability functions as a powerful form of pre-emptive compliance. The strategic calculation here is that organizations will choose to adopt this framework to demonstrate a commitment to accountability that exceeds current regulatory requirements, thereby gaining a competitive advantage and a potential "safe harbor" against future litigation, provided they do so correctly.

### **2.3 Asynchronous Auditing: The Creation of Immutable, Court-Admissible Logs**

A defining feature of the TML framework is its sophisticated logging mechanism, which is designed to produce immutable, court-admissible audit trails \[Q1, Q6\]. The Sacred Pause state, in particular, triggers the generation of "comprehensive moral reasoning documentation" that records every aspect of the AI's decision-making process \[Q2\]. These logs are created asynchronously, after the AI has delivered its immediate response, to ensure no performance slowdown for the user \[Q4\].  
The framework employs a combination of advanced security measures to ensure the integrity of these logs, including cryptographic signing with Hardware Security Modules (HSM) or Trusted Platform Modules (TPM), blockchain anchoring for every 1000 entries, and compliance with Write Once, Read Many (WORM) storage protocols \[Q10\]. The most radical legal assertion of the framework is that missing logs create an "irrebuttable presumption of maximum fault" \[Q5, Q6\]. This inverts the traditional burden of proof in civil litigation. Instead of a plaintiff having to prove that an AI system caused harm due to negligence, the absence of the required TML logs is considered automatic proof of organizational guilt \[Q6\]. This powerful deterrent is a core component of the framework's enforceability. The technical safeguards are critical to making this legal proposition viable, as they are designed to preempt any challenges to the authenticity or integrity of the audit trails in a court of law \[Q17\].

### **2.4 Scaling and Optimization: A Deep Dive into Storage and Performance**

The TML framework anticipates and addresses the practical challenges of scaling an exhaustive logging system to modern enterprise levels. It claims to be architected for billions of interactions through a combination of distributed processing and background logging \[Q18\]. A key optimization feature is "pattern recognition," which enables a claimed 90% storage reduction \[Q11, Q19\]. This is achieved by storing common ethical scenarios as reusable templates, with similar subsequent cases only referencing the template rather than generating a full, redundant log \[Q11\].  
For high-volume systems, this intelligent optimization maintains full audit capability while mitigating the storage burden \[Q19\]. The framework also offers a "lightweight implementation" for resource-constrained systems such as edge and IoT devices, ensuring its applicability across diverse technological environments \[Q20\]. These features demonstrate a mature understanding of real-world business constraints and are a strategic response to common objections regarding the operational feasibility of a comprehensive AI accountability system. The claims regarding storage efficiency and performance are presented as solutions to potential market resistance, positioning TML as a commercially viable and scalable product.

### **Table 1: TML Performance Metrics and Efficiency Gains**

| Metric | Traditional Binary | TML with Sacred Pause | Improvement |
| :---- | :---- | :---- | :---- |
| Harmful Decisions | 28% | 9% | 68% reduction 1 |
| Accuracy | 72% | 90% | 25% increase 1 |
| Human Trust Score | 3.2/5 | 4.6/5 | 44% increase 1 |
| Audit Compliance | 61% | 94% | 54% increase 1 |

The framework's self-reported performance metrics, validated by 50 ethics researchers, provide a clear business case for its adoption.1 The table above illustrates TML's purported ability to not only reduce harmful outputs and increase accuracy, but also to significantly improve human trust and audit compliance. These gains are presented as tangible returns on investment for organizations, translating the abstract concept of ethical behavior into concrete, measurable business outcomes.1

## **Part 3: TML's Legal and Regulatory Landscape**

### **3.1 A New Era of Accountability: Moving from Guidelines to Law**

The TML framework is not merely a technical standard; it is a proposed legal and regulatory model. While other frameworks like the NIST AI Risk Management Framework (NIST AI RMF) provide voluntary guidelines 7 and the EU AI Act establishes a broad, legally binding set of rules 8, TML carves out a unique space. It is a prescriptive technical standard that carries its own specified, severe legal consequences for non-compliance. This makes TML a "standard with teeth," as it mandates accountability through explicit penalties rather than relying on aspirational principles or broad, externally-enforced regulations \[Q30\].  
The framework is designed for coordination with existing regulations, such as the EU AI Act and GDPR, by providing the necessary technical infrastructure for risk assessment and explainable automated decision-making \[Q22\]. TML's core distinction lies in its focus on "enforceable accountability" with "real consequences" and the capability for criminal prosecution for violations \[Q30\]. This approach shifts the conversation from what is ethically advisable to what is legally required, creating a powerful incentive for voluntary adoption by organizations seeking to demonstrate a high standard of governance.

### **3.2 Criminal Penalties and Irrebuttable Presumption of Fault**

The TML framework proposes a level of legal deterrent that goes beyond most existing or proposed AI regulations. It specifies severe criminal penalties for violations, including 5 years of imprisonment for false attestation and 20 years for log tampering \[Q5\]. Furthermore, it holds executives personally liable for violations \[Q5\].  
The most aggressive and novel legal concept within the framework is the "irrebuttable presumption of maximum fault" for missing logs \[Q5, Q6\]. This is a radical proposition in legal terms, as it bypasses the need for a plaintiff to prove negligence or harm, instead making the absence of the audit trail automatic proof of guilt. This is a fundamental inversion of legal burden. While the legal viability of such a claim would be subject to jurisdictional review, it serves as a powerful deterrent against any attempt to hide or destroy evidence of an AI system's actions. The framework is, in essence, designed to build the legal case for the plaintiff directly into its technical architecture, ensuring that the necessary evidence for a negligence claim is either immutably preserved or that its absence is considered an admission of guilt.

### **3.3 The Lev Goukassian Memorial Fund: A Model for Victim Compensation**

An often-overlooked aspect of AI accountability is the post-harm support for victims. TML addresses this directly through the establishment of the Lev Goukassian Memorial Fund \[Q7\]. This fund allocates 30% of its resources to victim support, including providing emergency financial assistance for medical care, legal representation in negligence cases, and expert testimony to prove system failures \[Q7\].  
This feature demonstrates a holistic approach to accountability that extends beyond simply punishing the offender. By creating a dedicated mechanism for victim compensation and support, the framework addresses the full lifecycle of an AI-related incident, from prevention through the Sacred Pause to post-harm redress. This adds a crucial layer of credibility and moral purpose to the framework, signaling a commitment not just to corporate compliance but to genuine human welfare. It is a feature that could serve as a powerful public relations tool for any organization that voluntarily implements TML, demonstrating a commitment to their users that goes beyond regulatory obligations.

### **Table 2: Comparative Regulatory Analysis of TML, EU AI Act, and NIST RMF**

| Feature | Ternary Moral Logic (TML) | EU AI Act | NIST AI Risk Management Framework (RMF) |
| :---- | :---- | :---- | :---- |
| **Regulatory Status** | Technical Standard with proposed legal clauses; voluntary adoption with binding consequences | Legally binding, EU-wide regulation 8 | Voluntary guidelines, non-binding 7 |
| **Penalties** | Criminal (imprisonment) and financial penalties, executive liability \[Q5\] | Administrative fines up to 7% of global revenue 8 | No fines; non-compliance may lead to reputational or operational risks 7 |
| **Core Principles** | Enforceable Accountability, Transparency, Deliberate Moral Reasoning \[Q30, Q14\] | Risk-based classification, transparency, human oversight 7 | Trustworthy AI attributes (validity, safety, fairness, etc.) 10 |
| **Approach to Accountability** | Prescriptive, self-contained; shifts burden of proof with "irrebuttable presumption" of guilt \[Q5, Q6\] | Ex-ante (pre-market) and ex-post (post-market) obligations 7 | Principle-based guidance; promotes self-regulation and continuous improvement 11 |

This comparative analysis demonstrates TML's unique position in the global AI governance landscape. While the EU AI Act provides broad legal requirements and the NIST RMF offers flexible, non-binding guidance, TML is a singular model that combines the prescriptive nature of a technical standard with the legal severity of a regulatory act. The frameworkâ€™s willingness to propose criminal penalties and shift the legal burden of proof sets it apart, positioning it as a potentially more rigorous and aggressive accountability solution than either of its primary counterparts.

## **Part 4: Governance, Oversight, and Implementation**

### **4.1 The 11-Institution Governance Council: Authority and Limitations**

TML's credibility is bolstered by a proposed 11-institution governance council that includes leading academic, research, and international bodies such as Stanford, MIT, Harvard, the UN, and the WHO \[Q24\]. The association with such globally recognized institutions lends immediate authority to the framework.  
However, the authority of this council is carefully and strategically limited. The council's role is to provide "read-only access" for evidence review and expert analysis of AI system performance \[Q25\]. They are explicitly granted "No operational control over live systems" \[Q25\]. This is a critical design choice. It allows the framework to leverage the credibility and trust of prestigious institutions without those institutions assuming the immense legal and operational liability of direct enforcement or control. This model ensures the framework's continued integrity and oversight while insulating the governance body from the risks associated with daily operations.

### **4.2 Application Across Domains: A Multi-Sectoral Impact Analysis**

TML is designed as a universal accountability layer applicable to any AI system that affects human welfare \[Q15\]. The framework provides specific case studies for its application across critical sectors, including:

* **Healthcare:** A medical AI can use the Sacred Pause to recommend human specialist consultation rather than forcing a binary diagnosis when uncertainty is high.5  
* **Financial Markets:** The "Epistemic Hold" serves as a deliberative pause in algorithmic trading, preventing flash crashes and improving forecasting in ambiguous market conditions.5  
* **Autonomous Systems:** The framework can guide autonomous vehicles to distinguish between permissible and forbidden actions based on ethical curvature, even in chaotic situations.4  
* **Content Moderation:** TML enables a more nuanced approach to complex content decisions, moving beyond a simple allow/deny binary.5

The frameworkâ€™s claim of universal applicability is a key component of its value proposition. By addressing a diverse range of ethical dilemmas, TML positions itself as a foundational layer for responsible AI across all industries, rather than a niche, sector-specific solution. It also specifically claims to address systemic bias by logging demographic impacts and providing an investigative capability when discrimination is suspected \[Q16\].

### **4.3 Integrating TML with Existing Systems and Regulatory Compliance**

TML is architected to function as a modular "accountability layer" that can be integrated with any existing AI framework, including popular platforms like TensorFlow, PyTorch, and large language models \[Q23\]. The migration process is described as a systematic, phased deployment alongside existing systems to calibrate thresholds and validate minimal performance impact \[Q21\].  
This design choice is a strategic business decision. By positioning itself as a non-disruptive add-on, TML is a more palatable option for large enterprises than a complete technological overhaul. Its focus on regulatory coordination means it is designed to help organizations comply with existing mandates like the EU AI Act's transparency requirements and GDPR's automated decision-making rules, while also providing court-ready legal evidence for any litigation \[Q22\]. This seamless integration with existing systems and regulatory frameworks makes TML a compelling and practical option for companies navigating the complex landscape of global AI governance.

## **Part 5: Critical Analysis and Future Outlook**

### **5.1 Strengths of the TML Framework: Enforceability, Transparency, and Legal Accountability**

The TML framework's primary strength is its bold attempt to translate abstract ethical principles into a concrete, enforceable standard. It provides a clear path to accountability with "real consequences" \[Q30\], moving the conversation from what is an aspirational guideline to what is a mandatory, legally-backed requirement. By generating court-admissible evidence and proposing criminal penalties, TML offers a level of deterrence that is largely absent from other AI ethics frameworks. It also directly addresses the "black box" problem by providing a "complete 'glass box' visibility" into AI decision-making, which can build significant user trust and aid in regulatory compliance.12  
The framework also goes a step further by including a mechanism for victim support through the Memorial Fund \[Q7\]. This holistic approach, which addresses both the prevention of harm and the support for those who have been harmed, distinguishes TML as a comprehensive model for accountability.

### **5.2 Challenges and Open Questions: A Nuanced Look at Viability**

While TML's vision is compelling, its viability is subject to several key challenges and open questions. The framework relies heavily on self-reported performance metrics, such as the "68% reduction in harmful outputs" and "90% factual accuracy," which were reportedly validated by a cohort of ethics researchers.1 For TML to be adopted as a global standard, these claims will require rigorous, independent, third-party validation to ensure their methodological rigor and reproducibility.  
The framework's genesis from a single individual with a compressed development timeline also poses questions about its long-term sustainability. While the creator's personal narrative is a powerful call to action, it also makes the framework vulnerable to a single point of failure. The long-term funding and maintenance model beyond the creator's involvement and the Memorial Fund's scope are not fully specified. A crucial factor for widespread corporate and regulatory adoption will be the establishment of a robust, independent, and sustainable governance body that can maintain and evolve the framework for decades to come. The success of the framework ultimately rests on whether organizations are willing to voluntarily adopt a system that not only requires them to assume full liability for implementation failures but also exposes them to severe legal penalties that are not mandated by existing law.

### **5.3 Future Directions: TML's Role in the Development of AGI and Multi-Agent Systems**

The TML framework is not designed merely for today's AI systems. It is presented as a foundational layer for the development of future, more complex systems, including Artificial General Intelligence (AGI) and multi-agent AI \[Q28\]. The frameworkâ€™s design for distributed accountability, where each agent in a multi-agent system generates an individual "moral trace," demonstrates a forward-looking vision that anticipates the ethical complexities of tomorrow's AI ecosystems \[Q27\].  
By framing AGI not as a moral arbiter but as a "moral partner" 4, TML attempts to provide a critical accountability infrastructure that allows for "value learning transparency during development" and "democratic oversight of advanced AI capabilities" \[Q28\]. The framework positions itself as the necessary layer of ethical deliberation that will be required as AI systems become increasingly autonomous and interconnected, ensuring that AGI arrives with wisdom and accountability, not just raw intelligence.

## **Conclusion: A Strategic Assessment and Recommendations**

The Ternary Moral Logic (TML) Framework represents a bold and innovative model for AI accountability. It uniquely combines a philosophical call for ethical hesitation with a prescriptive technical and legal standard. By moving beyond aspirational principles to enforceable law, TML offers a compelling solution to the "black box" problem of AI ethics. Its focus on generating immutable, court-admissible evidence and its proposal of severe penalties for non-compliance create a powerful deterrent that could reshape the AI governance landscape.  
However, the framework's long-term success is not guaranteed. Its reliance on self-reported data and its aggressive legal propositions require independent validation and careful consideration by adopting organizations and policymakers.

### **Recommendations**

Based on this analysis, the following recommendations are provided for a strategic approach to the TML framework:

* **For Corporate Decision-Makers:** Organizations operating in high-stakes domains (e.g., healthcare, finance, autonomous systems) should consider TML as a potential tool to proactively reduce liability and build user trust. A pilot program should be initiated to test the framework's claims of performance gains and validate its operational feasibility within a controlled environment. Legal counsel and compliance officers should conduct a thorough analysis of the specific legal risks associated with the framework's strict liability clauses before committing to full-scale adoption.  
* **For Regulators and Policymakers:** Jurisdictions should study TML's model as a potential blueprint for future AI legislation. The framework's unique approach to evidence generation and its concept of "irrebuttable presumption of fault" offer a new path for effective enforcement. Regulators should focus on understanding the legal viability of these clauses and how a technical standard could be integrated into existing legal systems.  
* **For the AI Community:** The open-source nature of TML provides a unique opportunity for collaboration and independent review. Researchers and developers are encouraged to contribute to the framework's codebase and to conduct independent audits of its performance metrics and security claims. This community engagement is essential for moving TML from a visionary concept to a globally trusted and reliable standard for ethical AI.

#### **Works cited**

1. How a Terminal Diagnosis Inspired a New Ethical AI System | Balita sa MEXC, accessed September 1, 2025, [https://www.mexc.co/fil-PH/news/how-a-terminal-diagnosis-inspired-a-new-ethical-ai-system/68113](https://www.mexc.co/fil-PH/news/how-a-terminal-diagnosis-inspired-a-new-ethical-ai-system/68113)  
2. How a Terminal Diagnosis Inspired a New Ethical AI System \- HackerNoon, accessed September 1, 2025, [https://hackernoon.com/how-a-terminal-diagnosis-inspired-a-new-ethical-ai-system](https://hackernoon.com/how-a-terminal-diagnosis-inspired-a-new-ethical-ai-system)  
3. How a Terminal Diagnosis Inspired a New Ethical AI System | MEXC News, accessed September 1, 2025, [https://www.mexc.co/en-IN/news/how-a-terminal-diagnosis-inspired-a-new-ethical-ai-system/68113](https://www.mexc.co/en-IN/news/how-a-terminal-diagnosis-inspired-a-new-ethical-ai-system/68113)  
4. FractonicMind/TernaryMoralLogic: Implementing Ethical Hesitation in AI Systems \- GitHub, accessed September 1, 2025, [https://github.com/FractonicMind/TernaryMoralLogic](https://github.com/FractonicMind/TernaryMoralLogic)  
5. How Ternary Moral Logic is Teaching AI to Think, Feel, and Hesitate \- Medium, accessed September 1, 2025, [https://medium.com/ternarymorallogic/beyond-binary-how-ternary-moral-logic-is-teaching-ai-to-think-feel-and-hesitate-73de201e084e](https://medium.com/ternarymorallogic/beyond-binary-how-ternary-moral-logic-is-teaching-ai-to-think-feel-and-hesitate-73de201e084e)  
6. The AI That Learned to Hesitate: How One Question Sparked a Revolution in Machine Ethics | by Lev Goukassian | Jul, 2025 | Medium, accessed September 1, 2025, [https://medium.com/@leogouk/the-ai-that-learned-to-hesitate-how-one-question-sparked-a-revolution-in-machine-ethics-8535d375a70c](https://medium.com/@leogouk/the-ai-that-learned-to-hesitate-how-one-question-sparked-a-revolution-in-machine-ethics-8535d375a70c)  
7. EU AI Act vs NIST AI RMF A Practical Guide to AI Compliance in 2025 \- AI Governance Blog, accessed September 1, 2025, [https://blog.cognitiveview.com/eu-ai-act-vs-nist-ai-rmf-a-practical-guide-to-ai-compliance-in-2025/](https://blog.cognitiveview.com/eu-ai-act-vs-nist-ai-rmf-a-practical-guide-to-ai-compliance-in-2025/)  
8. Article 99: Penalties | EU Artificial Intelligence Act, accessed September 1, 2025, [https://artificialintelligenceact.eu/article/99/](https://artificialintelligenceact.eu/article/99/)  
9. Penalties of the EU AI Act: The High Cost of Non-Compliance \- Holistic AI, accessed September 1, 2025, [https://www.holisticai.com/blog/penalties-of-the-eu-ai-act](https://www.holisticai.com/blog/penalties-of-the-eu-ai-act)  
10. NIST vs ISO \- Compare AI Frameworks \- ModelOp, accessed September 1, 2025, [https://www.modelop.com/ai-governance/ai-regulations-standards/nist-vs-iso](https://www.modelop.com/ai-governance/ai-regulations-standards/nist-vs-iso)  
11. NIST AI Risk Management Framework: A tl;dr \- Wiz, accessed September 1, 2025, [https://www.wiz.io/academy/nist-ai-risk-management-framework](https://www.wiz.io/academy/nist-ai-risk-management-framework)  
12. Legal AI Audit Trails: Designing for Traceability \- Law.co, accessed September 1, 2025, [https://law.co/blog/legal-ai-audit-trails-designing-for-traceability](https://law.co/blog/legal-ai-audit-trails-designing-for-traceability)