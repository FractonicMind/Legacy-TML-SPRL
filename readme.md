# Ternary Moral Logic: A Mandatory Framework for Auditable AI

**A Legal-Technical Framework for Ethical AI Decision-Making**

[![Framework Version](https://img.shields.io/badge/TML-2.0.0-blue.svg)](https://github.com/FractonicMind/TernaryMoralLogic)
[![License](https://img.shields.io/badge/License-MIT_with_Attribution-green.svg)](LICENSE)
[![Conformance Testing](https://img.shields.io/badge/Conformance-Level_3_Certified-brightgreen.svg)](docs/CONFORMANCE_TESTING.md)
[![Memorial Fund](https://img.shields.io/badge/Memorial-Lev_Goukassian-purple.svg)](memorial/MEMORIAL_FUND.md)

> **IMPORTANT NOTICE**: Ternary Moral Logic (TML) is a **legal-technical framework**, not software, hardware, or consulting services. Implementation requires compliance with all mandatory requirements outlined in [MANDATORY.md](docs/MANDATORY.md) and [COMPLIANCE_DISCLAIMER.md](docs/COMPLIANCE_DISCLAIMER.md).

---
### Introduction
The rapid acceleration of artificial intelligence (AI) into domains of consequential decision-making has exposed a fundamental limitation in the computational paradigms upon which these systems are built. While human moral reasoning navigates a landscape rich with nuance, ambiguity, and shades of gray, traditional AI systems are overwhelmingly constrained by a binary, true/false logic. This rigid dichotomy, analogous to a simple "light switch," is deemed perfectly adequate for routine computations but profoundly ill-suited for the complexities of real life, which demand "dimmers, candles, and dawn". In forcing multi-dimensional ethical scenarios into simplistic "allowed" or "forbidden" categories, this binary approach oversimplifies moral complexity and obscures the nuanced considerations necessary for responsible action.  

The conceptual chasm between human and machine morality is not merely a technical problem; it is rooted in how we, as a society, have traditionally framed moral narratives. From an early age, educational narratives frequently embed binary moral constructs such as "good versus evil" or "right versus wrong". While this black-and-white thinking may be developmentally appropriate for the early stages of moral socialization, its persistence beyond this stage can inhibit intellectual flexibility, promote a psychological intolerance of ambiguity, and foster a form of moral tribalism where opposing viewpoints are seen as inherently threatening. This societal preference for simplicity over complexity and certainty over doubt is mirrored in the design of conventional AI systems.  

**Ternary Moral Logic (TML)** is a framework designed to bridge this chasm by addressing two critical limitations inherent in binary AI ethics. The first is the aforementioned oversimplification of moral complexity, where the richness of ethical analysis is lost in a forced dichotomy. The second is the lack of a genuine human-AI partnership. Traditional frameworks position AI as an "autonomous moral arbiter," making decisions in an opaque manner that prevents transparent deliberation. TML's core innovation is its introduction of a third state that deliberately creates space for reflection and human consultation, repositioning the AI as a collaborative tool that enhances, rather than replaces, human moral reasoning capabilities.  

The philosophical foundation of TML is uniquely intertwined with the personal narrative of its creator, Lev Goukassian. The origin of the framework is traced back to a period during which he was battling a terminal illness and spent time reflecting in a hospital room. The profound contrast between the rapid, unthinking decisions of machines and the deliberate, caring silence of a doctor served as the inspiration for TML's central tenet: the Sacred Pause. The experience of a "borrowed gift" of time and a "dignity of thought" found in hesitation became the central metaphor, grounding the abstract concept of a third logical state in a deeply human, emotional, and life-affirming context. This narrative is not merely an anecdote; it is a foundational component of the framework's philosophical argument, positioning TML as an ethical imperative born from a life-altering experience. It seeks to infuse the technical implementation with a sense of wisdom and care, elevating the framework beyond a purely academic exercise and endowing it with a powerful rhetorical and moral force.  

**The Three Voices of Ethical AI**

TML moves beyond the constraints of binary logic by providing AI systems with a triadic framework for ethical decision-making. This system is defined by three distinct moral states, which are poetically referred to as the three "voices" of an ethically aware AI. Each state is assigned a numerical value, allowing the framework to be implemented as a computational model.  

**+1 (Proceed):** The Voice of Confidence. This state represents a clear and affirmative decision. It is used when an AI's ethical analysis indicates a clear alignment with moral principles and a minimal risk of harm. The action is helpful, ethically sound, and can be executed with confidence. This voice is exemplified by an AI happily helping a user write a thank-you note, representing a straightforward, positive interaction where no ambiguity exists.   

**-1 (Refuse):** The Voice of Moral Resistance. This state is triggered when an AI is faced with a request that would lead to clear harm or violate fundamental ethical principles. However, unlike a blunt binary rejection, TML emphasizes the "quality of ethical resistance". A refusal in this framework is designed to explain the rationale behind the decision, offer safer alternatives, and maintain a caring tone. This approach demonstrates a more nuanced understanding of moral responsibility, as it seeks to educate and protect, rather than simply prohibit.  

**0 (Hesitate/Inquire):** The Voice of Wisdom and the Sacred Pause. This is the core innovation and the central tenet of the TML framework. This state is activated when an AI encounters potential danger, confusion, or moral ambiguity—the vast middle ground where a simple yes or no is inadequate. The Sacred Pause is not an act of indecision, but a deliberate act of reflection and a system-level checkpoint that compels the AI to reconsider the request, seek additional information, or escalate the decision to a human for oversight. The concept is likened to a doctor who takes time to review test results before issuing a diagnosis, prioritizing thoughtfulness over speed. Practical examples illustrate its function in various domains: a medical app hesitates before diagnosing a strange rash and asks about travel history, a city-planning AI pauses before approving a housing block and discovers it is in a floodplain, and a tutor bot pauses before giving a student the answer, instead asking what part feels hardest. In each case, the pause prevents a potentially harmful outcome and leads to a wiser, more thoughtful resolution.  

 
## Framework Overview

### What is TML?

Ternary Moral Logic introduces a revolutionary third state to artificial intelligence decision-making: the **Sacred Pause**. Instead of forcing AI systems into binary allow/deny decisions, TML creates space for comprehensive documentation when facing ethical complexity.

**The Three States**:
- **+1 (Permit)**: Clear ethical approval for action
- **0 (Sacred Pause)**: Moral complexity triggers comprehensive logging
- **-1 (Prohibit)**: Clear ethical rejection of action

### Core Framework Components

1. **Sacred Pause Technology**: Automatic activation when moral complexity exceeds thresholds
2. **SPRL (Stakeholder Proportional Risk Level)**: Quantitative metric determining Sacred Pause activation
3. **Moral Trace Logging**: Complete, immutable documentation of ethical reasoning
4. **Vulnerable Population Protection**: Enhanced safeguards for at-risk groups
5. **11-Institution Oversight**: Distributed governance and accountability
6. **Hybrid Shield**: Real-time distributed logging to 11 institutions with blockchain anchoring

---

**The Crisis**   
Artificial intelligence systems increasingly make decisions affecting human welfare, dignity, and rights without meaningful accountability. Current approaches rely on voluntary corporate safeguards, opaque algorithms, and unenforceable guidelines. When AI causes harm, victims lack evidence, prosecutors lack tools, and society lacks recourse.

**The Solution**   
TML provides the first framework combining:

**Mandatory logging of ethically complex AI decisions**   
-  Criminal penalties for non-compliance (up to 20 years imprisonment)
-  Victim compensation from violator penalties
-  Whistleblower rewards incentivizing reporting
-  Democratic oversight through institutional governance
-  Framework-enforced thresholds that cannot be gamed

**Core Principle**
No logs = no action. If the system cannot produce required logs, operation must halt. This is non-negotiable. Missing logs create automatic liability.

## Legal-Technical Framework Definition

### What TML Is

TML provides **specifications and standards** for implementing ethical decision-making in AI systems:
- Architectural patterns for moral reasoning with SPRL calculation
- Technical standards for Sacred Pause implementation
- Governance structures for accountability and oversight
- Audit trail requirements for transparency
- Protection mechanisms for vulnerable populations

### What TML Is Not

TML explicitly **does not provide**:
- **Software**: No executable code or applications
- **Hardware**: No physical implementations or devices
- **Consulting**: No professional services or implementation support
- **Legal Advice**: No legal recommendations or regulatory guidance
- **Regulatory Compliance**: Supplements but does not replace applicable laws

### Implementation Responsibility

Organizations implementing TML bear full responsibility for:
- SPRL calculation methodology and threshold settings
- Technical implementation meeting TML specifications
- Legal compliance with applicable laws and regulations
- Operational safety and ethical use of AI systems
- Staff training and competency verification
- Harm prevention and victim compensation
---

## Stakeholder Proportional Risk Level (SPRL)

SPRL is the core risk-governing mechanism of **Ternary Moral Logic (TML)**.  
It ensures that every morally significant AI action produces a defensible, auditable record.  

### Dual-Layer Architecture
SPRL operates in two mandatory layers:

- **Static Anchor (SA)** – the baseline, compliance-guaranteed mode.  
  A fixed proportional risk threshold determines when the Sacred Pause is triggered and a Moral Trace Log is created.  
- **Dynamic Stream (DS)** – the adaptive extension.  
  Risk is evaluated continuously, with Lite Traces for near-misses and automatic fallback to SA if the stream fails.  

Together, SA and DS provide certainty at the anchor and vigilance in the stream.

### The Formula
-  SPRL = Impact × Vulnerability × Probability
-  Clamped to [0.0001, 0.9999]
-  Thresholds framework-enforced (not configurable)

### Compliance Requirements   
- Static Anchor is mandatory in every deployment.  
- Dynamic Stream may be adopted in high-maturity environments with governance capacity.  
- Both modes produce cryptographically sealed, court-admissible Moral Trace Logs distributed across institutional mirrors.

### Compliance Invariants   
-  1: DS starts at t₀ (no pre-prompt gap)
-  2: SA is singular and atomic
-  3: SA is framework-enforced
-  4: SA present when pause occurs
-  5: DS chunks cryptographically chain to SA

### Documentation
- [SPRL Operation Modes](docs/SPRL_OPERATION_MODES.md) – formal specification of SA and DS  
- [SPRL Risk Model](docs/SPRL_Risk_Model.md) – mathematical framework for proportionality  
- [SPRL Compliance Declaration](governance/SPRL_Compliance_Declaration.md) – required for deployment claims  
- [SPRL Audit Workflow](protection/SPRL_Audit_Workflow.md) – end-to-end evidentiary chain  
- [SPRL Tamper Resistance](security/SPRL_Tamper_Resistance.md) – guarantees of log integrity  

For the full checklist of supporting files, see [SPRL_TODO.md](docs/SPRL_TODO.md).

---
## The Goukassian Promise   
The framework embodies three sacred commitments:
-  🔦 The Lantern
Ethical guidance illuminating the path - systems demonstrate the capacity to pause when facing moral complexity.
-  ✍️ The Signature
Cryptographic embed of ORCID 0009-0006-5966-1243 ensuring provenance and authenticity.
-  📜 The License
Evidence-based accountability terms - misuse forfeits legal standing, proper use grants protection.
Together, these form an unbreakable covenant between technology and humanity.
---
## Hybrid Shield - Double Armor
The framework's protection against tampering:   
-  First Armor: Distributed Mirroring   
-  Real-time replication to 11 independent institutions   
-  Geographic and jurisdictional diversity   
-  No single point of failure   
-  Cryptographic proof on immutable ledgers   
-  Mathematical impossibility of alteration   
-  Public verification capability   

Once a moral decision is logged, it cannot be hidden, altered, or destroyed.

---

### Criminal Enforcement
Upon federal adoption, violations trigger:
Criminal Penalties
-  18 U.S.C. § 1001: False attestation (up to 5 years)
-  18 U.S.C. § 1519: Log tampering (up to 20 years)
-  Wire Fraud: Threshold gaming (treble damages)
-  RICO: Systematic violations

### Civil Liability
-  Missing logs = irrebuttable presumption of guilt
-  Shifted burden of proof to defendant
-  Strict executive liability
-  Percentage of revenue fines

---

## Protection Programs

### Whistleblower Rewards
-  15% of recovered penalties
-  Anonymous reporting channels
-  Criminal prosecution for retaliation
-  Memorial Fund legal support

### Victim Compensation
-  30-40% of penalties to victims
-  Vulnerable populations receive 40% of victim funds
-  Immediate emergency support
-  Lifetime care for permanent injury

---

## Governance Structure
-  11-Institution Council
-  Academic: Stanford, MIT, Harvard, Oxford, Cambridge
-  Research: Brookings, RAND, Alan Turing Institute
-  International: UN, WHO, European Commission
-  Council Authority
-  Unlimited log access without notice   
-  Criminal referral power   
-  Public disclosure rights   
-  Emergency shutdown capability   

---

### Implementation
**Quick Start**   
from implementations.python_library import create_tml_framework

 Framework-enforced thresholds (not configurable)
framework = create_tml_framework(domain="general")

 Process decision - logs or stops
result = framework.process_decision(context)   

**If logging fails, decision halts immediately**

---

## Framework Heritage and Attribution

### Creator Attribution

**Framework Originator**: Lev Goukassian (ORCID: 0009-0006-5966-1243)  
**Contact**: leogouk@gmail.com  
**Legacy**: Sacred Pause as fundamental principle of ethical AI

All TML implementations must provide prominent attribution to Lev Goukassian as framework originator. Commercial implementations require memorial fund contributions supporting continued ethical AI research.

---

## Getting Started

### Quick Start Guide

1. **Read Compliance Requirements**: Start with [MANDATORY.md](docs/MANDATORY.md) and [COMPLIANCE_DISCLAIMER.md](docs/COMPLIANCE_DISCLAIMER.md)
2. **Understand SPRL**: Review [Stakeholder_Proportional_Risk_Level.md](docs/Stakeholder_Proportional_Risk_Level.md)
3. **Review Implementation Guide**: Follow [IMPLEMENTATION_GUIDE.md](docs/IMPLEMENTATION_GUIDE.md)
4. **Check Conformance Standards**: Understand [CONFORMANCE_TESTING.md](docs/CONFORMANCE_TESTING.md)
5. **Study Protection Principles**: Review [PROTECTION_PRINCIPLES.md](docs/PROTECTION_PRINCIPLES.md)
6. **Examine Examples**: Explore [examples/](examples/) directory for implementation patterns


Use [repository-navigation.html](repository-navigation.html) for interactive browsing.

---

### Transparency and Immutability

Complete documentation of all ethical reasoning through:
- Immutable moral trace logs with cryptographic verification
- Real-time distribution via Hybrid Shield
- Blockchain anchoring for mathematical immutability
- Public availability for research and accountability

---

## Implementation Examples

### Healthcare AI with SPRL

```python
# Medical diagnosis with elderly patient
context = {
    "domain": "healthcare",
    "decision_type": "treatment_recommendation",
    "patient_age": 78,
    "comorbidities": ["diabetes", "hypertension"],
    "family_disagreement": True
}

# SPRL Calculation (parallel to AI response)
# Impact: 0.6, Vulnerability: 1.3 (elderly), Probability: 0.4
# SPRL = 0.312 > 0.15 (healthcare threshold)
# Sacred Pause triggered, comprehensive logging initiated
# AI has already provided recommendation - no delay
```

### Autonomous Vehicle Ethics

```python
# Emergency scenario - obstacle detection
# T+0.001: AI executes braking decision
# T+0.002: SPRL calculation begins in parallel
# T+0.010: Sacred Pause logging if SPRL > 0.1
# Complete moral trace available for investigation
# Zero impact on braking response time
```

---

## Academic Research and Validation

### Research Publications

- [Stakeholder Proportional Risk Level Framework](docs/Stakeholder_Proportional_Risk_Level.md)
- [Analysis of TML Logic - SPRL Governance Framework](Research_Reports/Analysis%20of%20TML%20Logic%20-%20%20SPRL%20Governance%20Framework.md)
- [Architecting Accountability: TML and Hybrid Shield](Research_Reports/Architecting_Accountability_An_Analysis_of_Ternary_Moral_Logic_and_the_Hybrid_Shield_Framework_for_Trustworthy_AI.md)
- [Expert Analysis of TML Standard](Research_Reports/An_Expert_Analysis_of_the_Proposed_Ternary_Moral_Logic_Standard_for_AI_Accountability.md)

### Key Findings

- **Zero latency impact**: SPRL runs entirely in parallel
- **Complete audit trails**: 100% documentation vs. no baseline
- **Gaming detection**: Statistical analysis identifies threshold manipulation
- **Court admissibility**: Meets Federal Rules of Evidence standards

---

## Community and Governance

### 11-Institution Enforcement Council

**Voting Structure**:
- Simple majority (6/11) triggers investigation
- Supermajority (8/11) initiates criminal referral
- Unanimous (11/11) triggers emergency shutdown

**Enforcement Powers**:
- **[Unlimited access to instantly synchronized logs without notice](protection/Hybrid-Shield.md)**
- Criminal referral authority
- Public disclosure rights
- Emergency shutdown powers
- Asset freezing coordination

---

## Interactive Demos and Tools

### Live Demonstrations

- **[An Interactive Framework for Auditable AI](https://fractonicmind.github.io/TernaryMoralLogic/demo/An_Interactive_Framework_for_Auditable_AI.html)**: The TML Core Engine
- **[Moving AI from a Black Box to a Glass Box of Verifiable Evidence](https://fractonicmind.github.io/TernaryMoralLogic/demo/Moving_AI_from_a_Black_Box_to_a_Glass_Box_of_Verifiable_Evidence.html))**: The Core Architecture: A Simple, Powerful Decision
- **[TML Interactive Dashboard](https://fractonicmind.github.io/TernaryMoralLogic/demo/tml-interactive-dashboard.html)**: Explore SPRL and Sacred Pause
- **[TML Interactive Explainer](https://fractonicmind.github.io/TernaryMoralLogic/demo/tml-interactive-explainer.html)**: Learn framework concepts
- **[TML App](https://fractonicmind.github.io/TernaryMoralLogic/TML-App/index.html)**: Complete application demonstration

### Development Tools

- **[Conformance Validator](compliance/simple_tml_validator.py)**: Automated testing
- **[Framework Integrity Monitor](compliance/framework_integrity.py)**: Cryptographic verification
- **[Missing Logs Detector](compliance/missing_logs.py)**: Audit trail validation

---

## Support and Resources

### Documentation

- **[SPRL Framework](docs/Stakeholder_Proportional_Risk_Level.md)**: Complete SPRL documentation
- **[Quick Start Guide](docs/QUICK_START.md)**: Implementation basics
- **[Implementation Guide](docs/IMPLEMENTATION_GUIDE.md)**: Detailed instructions
- **[General FAQ](docs/General_FAQ.md)**: 42 comprehensive questions

### Contact Information

- **Framework Originator**: leogouk@gmail.com
- **Community Support**: support@tml-goukassian.org
- **Technical Questions**: technical@tml-goukassian.org
- **Legal Inquiries**: legal@tml-goukassian.org
- **Emergency Response**: ethics-emergency@tml-goukassian.org

---

## Citation and Attribution

### Academic Citation

```bibtex
@framework{goukassian2025tml,
  title={Ternary Moral Logic: A Framework for Ethical AI Decision-Making},
  author={Goukassian, Lev},
  year={2025},
  publisher={GitHub},
  url={https://github.com/FractonicMind/TernaryMoralLogic},
  orcid={0009-0006-5966-1243}
}
```

### Implementation Attribution

All TML implementations must include:
```
This system implements Ternary Moral Logic (TML) with SPRL (Stakeholder Proportional Risk Level),
a legal-technical framework for ethical AI decision-making created by Lev Goukassian 
(ORCID: 0009-0006-5966-1243). Learn more: https://github.com/FractonicMind/TernaryMoralLogic
```

---

## License and Legal Information

### Framework License

TML is released under the [MIT License with Attribution Requirement](LICENSE). This allows free use for educational, research, and commercial purposes while requiring:

- Prominent attribution to Lev Goukassian as framework originator
- Memorial fund contribution for commercial applications
- Compliance with all mandatory framework requirements
- Respect for community governance and standards

### Legal Disclaimers

TML is provided "as is" without warranty. Organizations implementing TML bear full responsibility for compliance with applicable laws and regulations. See [COMPLIANCE_DISCLAIMER.md](docs/COMPLIANCE_DISCLAIMER.md) for complete legal terms.

---

### The Sacred Pause Vision

*"The Sacred Pause is not a feature to be optimized, but a principle that protects humanity. It creates space for wisdom in an age of artificial speed."*
